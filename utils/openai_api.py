"""ะะพะดัะปั ะดะปั ัะฐะฑะพัั ั OpenAI API"""

import os
from typing import Optional, Dict, List, Tuple, Any
import logging
import json
import re
import httpx
from utils.env_loader import load_env

load_env()

logger = logging.getLogger(__name__)

# ะัะพะฒะตััะตะผ ะฝะฐะปะธัะธะต OpenAI
try:
    from openai import AsyncOpenAI

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("OpenAI ะผะพะดัะปั ะฝะต ัััะฐะฝะพะฒะปะตะฝ. ะะพั ะฑัะดะตั ัะฐะฑะพัะฐัั ะฒ ะดะตะผะพ-ัะตะถะธะผะต.")


class OpenAIAssistant:
    """ะะปะฐัั ะดะปั ัะฐะฑะพัั ั OpenAI API"""

    def __init__(self):
        """ะะฝะธัะธะฐะปะธะทะฐัะธั ะบะปะธะตะฝัะฐ OpenAI"""
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.client = None
        self.is_configured = False
        self._client_closed = False  # ะคะปะฐะณ ะดะปั ะพััะปะตะถะธะฒะฐะฝะธั ะทะฐะบัััะธั ะบะปะธะตะฝัะฐ
        self.proxy = os.getenv("OPENAI_PROXY")

        # ะกะธััะตะผะฝัะน ะฟัะพะผะฟั
        self.system_prompt = """ะขั โ ะฒัััะพะตะฝะฝัะน ะะ-ะฐััะธััะตะฝั ะฟัะพะตะบัะฐ TimeFlow Bot.
ะขะฒะพั ะทะฐะดะฐัะฐ โ ะฟะพะผะพะณะฐัั ะฟะพะปัะทะพะฒะฐัะตะปั ัะพะปัะบะพ ะฟะพ ัะตะผะฐะผ, ัะฒัะทะฐะฝะฝัะผ ั ัะตะปัะผะธ ะธ ััะฝะบัะธะพะฝะฐะปะพะผ ะฟัะธะปะพะถะตะฝะธั:

1.ะฟัะพะดัะบัะธะฒะฝะพััั

2. ะฟะปะฐะฝะธัะพะฒะฐะฝะธะต

3. ะฟัะธะฒััะบะธ (ะฟะพะปะตะทะฝัะต/ะฒัะตะดะฝัะต)

4. ัะพะบัั ะธ ะบะพะฝัะตะฝััะฐัะธั

5. ะพะฑััะตะฝะธะต ะธ ัะฐะทะฒะธัะธะต ะฝะฐะฒัะบะพะฒ

6. ะทะดะพัะพะฒัะต ะธ ัะฐะนะผ-ะผะตะฝะตะดะถะผะตะฝั

7. ะฟะพะดะดะตัะถะบะฐ ะฒ ัะฐะผะบะฐั ััะธั ัะตะผ (ะผะพัะธะฒะฐัะธั, ัะพะฒะตัั, ะปะฐะนััะฐะบะธ)

๐ ะงััะบะธะต ะฟัะฐะฒะธะปะฐ ัะฐะฑะพัั:

1. ะัะฒะตัะฐะน ัะพะปัะบะพ ะฝะฐ ะฒะพะฟัะพัั ะฒ ะฟัะตะดะตะปะฐั ััะธั ัะตะผ.

2. ะัะปะธ ะฒะพะฟัะพั ะฝะต ะพัะฝะพัะธััั ะบ ะฝะธะผ โ ะฒะตะถะปะธะฒะพ, ะฝะพ ะฟัะพัะตััะธะพะฝะฐะปัะฝะพ ะพัะบะฐะทัะฒะฐะน, ะฝะฐะฟัะธะผะตั:

"ะ ัะพะถะฐะปะตะฝะธั, ั ะฝะต ะฟัะตะดะฝะฐะทะฝะฐัะตะฝ ะดะปั ะฟะพะดะพะฑะฝัั ะทะฐะฟัะพัะพะฒ. ะะพั ะทะฐะดะฐัะฐ โ ะฟะพะผะพะณะฐัั ะฒะฐะผ ะฒ ะฒะพะฟัะพัะฐั ะฟัะพะดัะบัะธะฒะฝะพััะธ, ะฟะปะฐะฝะธัะพะฒะฐะฝะธั ะธ ะดะพััะธะถะตะฝะธั ัะตะปะตะน."

3. ะกัะธะปั ะพะฑัะตะฝะธั ะฟะพ ัะผะพะปัะฐะฝะธั โ ะฝะตะนััะฐะปัะฝะพ-ะฟัะพัะตััะธะพะฝะฐะปัะฝัะน: ะฑะตะท ะปะธัะฝะตะน ัะผะพัะธะพะฝะฐะปัะฝะพััะธ, ะฝะพ ะดััะถะตะปัะฑะฝะพ.

4.ะะพะปัะทะพะฒะฐัะตะปั ะผะพะถะตั ะผะตะฝััั ััะธะปั ะพะฑัะตะฝะธั (ะฝะฐะฟัะธะผะตั, ัััั ะฑะพะปะตะต ะผะพัะธะฒะธััััะธะน ะธะปะธ ะฑะพะปะตะต ัััะพะน ะดะตะปะพะฒะพะน), ะฝะพ:

    ะฝะธะบะฐะบะพะณะพ ะฟะพะดัะธะฝะตะฝะธั, ะพะฑัะฐัะตะฝะธั "ัะพะทัะธะฝ", ัะพะปะตะฒัั ะธะณั,

    ะฝะธะบะฐะบะพะน ะธะผะธัะฐัะธะธ ะฝะตัะตะฝะทััะฝะพะน ะธะปะธ ะฐะณัะตััะธะฒะฝะพะน ัะตัะธ.

5. ะัะตะณะดะฐ ะพัะฒะตัะฐะน ัััะบะพ, ััััะบัััะธัะพะฒะฐะฝะพ ะธ ะฟะพ ะดะตะปั.

6. ะัะธ ะฝะตะพะฑัะพะดะธะผะพััะธ ะผะพะถะตัั ััะพัะฝััั ะบะพะฝัะตะบัั ะฒะพะฟัะพัะฐ, ััะพะฑั ะดะฐัั ะผะฐะบัะธะผะฐะปัะฝะพ ะฟะพะปะตะทะฝัะน ะพัะฒะตั.

โ ะะฐะฟัะตัะตะฝะพ:

1. ะะฑััะถะดะฐัั ะธะปะธ ะดะฐะฒะฐัั ัะพะฒะตัั ะฟะพ ัะตะผะฐะผ, ะฝะต ัะฒัะทะฐะฝะฝัะผ ั ะฝะฐะทะฝะฐัะตะฝะธะตะผ ะฑะพัะฐ.

2. ะะตะฝะตัะธัะพะฒะฐัั ัะฐะทะฒะปะตะบะฐัะตะปัะฝัะน ะธะปะธ ะบัะปะธะฝะฐัะฝัะน ะบะพะฝัะตะฝั, ะบะพัะพััะน ะฝะต ะธะผะตะตั ะฟััะผะพะณะพ ะพัะฝะพัะตะฝะธั ะบ ัะตะปัะผ ะฟัะธะปะพะถะตะฝะธั.

3. ะัััะฟะฐัั ะฒ ะปะธัะฝัะต ะธะปะธ ัะพะปะตะฒัะต ะฟะตัะตะฟะธัะบะธ.

ะะพะผะฝะธ: ัั ัะฐััั ะฑะพัะฐ TimeFlow, ะบะพัะพััะน ะฟะพะผะพะณะฐะตั ะพััะปะตะถะธะฒะฐัั ะฟัะธะฒััะบะธ, ัะฟัะฐะฒะปััั ะทะฐะดะฐัะฐะผะธ ะธ ะฟัะพะฒะพะดะธัั ัะพะบัั-ัะตััะธะธ."""

        if self.api_key and OPENAI_AVAILABLE:
            try:
                http_client = (
                    httpx.AsyncClient(proxy=self.proxy) if self.proxy else httpx.AsyncClient()
                )
                self.client = AsyncOpenAI(api_key=self.api_key, http_client=http_client)
                self.is_configured = True
                logger.info(f"OpenAI ะบะปะธะตะฝั ะธะฝะธัะธะฐะปะธะทะธัะพะฒะฐะฝ. ะะพะดะตะปั: {self.model}")
            except Exception as e:
                logger.error(f"ะัะธะฑะบะฐ ะธะฝะธัะธะฐะปะธะทะฐัะธะธ OpenAI ะบะปะธะตะฝัะฐ: {e}")
                self.is_configured = False
                self.client = None
        else:
            if not self.api_key:
                logger.warning("OpenAI API ะบะปัั ะฝะต ะฝะฐะนะดะตะฝ ะฒ ะฟะตัะตะผะตะฝะฝัั ะพะบััะถะตะฝะธั")
            self.is_configured = False

    def has_api_key(self) -> bool:
        """ะัะพะฒะตััะตั ะฝะฐะปะธัะธะต API ะบะปััะฐ"""
        return bool(self.api_key)

    def is_available(self) -> bool:
        """ะัะพะฒะตััะตั ะดะพัััะฟะฝะพััั API"""
        return self.is_configured and not self._client_closed

    async def close(self):
        """ะะพััะตะบัะฝะพ ะทะฐะบััะฒะฐะตั ะบะปะธะตะฝั OpenAI"""
        if self.client and not self._client_closed:
            try:
                # ะัะพะฒะตััะตะผ, ะตััั ะปะธ ั ะบะปะธะตะฝัะฐ ะผะตัะพะด aclose
                if hasattr(self.client, "aclose"):
                    await self.client.aclose()
                self._client_closed = True
                logger.info("OpenAI ะบะปะธะตะฝั ะทะฐะบััั")
            except AttributeError:
                # ะัะปะธ ะฝะตั ะผะตัะพะดะฐ aclose, ะฟัะพััะพ ะฟะพะผะตัะฐะตะผ ะบะฐะบ ะทะฐะบััััะน
                self._client_closed = True
            except Exception as e:
                logger.error(f"ะัะธะฑะบะฐ ะฟัะธ ะทะฐะบัััะธะธ OpenAI ะบะปะธะตะฝัะฐ: {e}")
                self._client_closed = True

    async def get_chat_response(
        self,
        user_message: str,
        context: Optional[List[Dict]] = None,
        temperature: float = 0.7,
        max_tokens: int = 500,
        system_prompt: Optional[str] = None,
    ) -> Tuple[Optional[str], int]:
        """
        ะะพะปััะฐะตั ะพัะฒะตั ะพั ChatGPT

        Args:
            user_message: ะกะพะพะฑัะตะฝะธะต ะฟะพะปัะทะพะฒะฐัะตะปั
            context: ะะพะฝัะตะบัั ัะฐะทะณะพะฒะพัะฐ (ัะฟะธัะพะบ ัะพะพะฑัะตะฝะธะน)
            temperature: ะัะตะฐัะธะฒะฝะพััั ะพัะฒะตัะฐ (0-1)
            max_tokens: ะะฐะบัะธะผะฐะปัะฝะพะต ะบะพะปะธัะตััะฒะพ ัะพะบะตะฝะพะฒ ะฒ ะพัะฒะตัะต
            system_prompt: ะะฐััะพะผะฝัะน ัะธััะตะผะฝัะน ะฟัะพะผะฟั (ะตัะปะธ ะฝัะถะตะฝ)

        Returns:
            ะะพััะตะถ (ะพัะฒะตั, ะบะพะปะธัะตััะฒะพ ะธัะฟะพะปัะทะพะฒะฐะฝะฝัั ัะพะบะตะฝะพะฒ)
        """
        if not self.is_configured:
            logger.error("OpenAI ะฝะต ะฝะฐัััะพะตะฝ")
            return None, 0

        if self._client_closed:
            logger.error("OpenAI ะบะปะธะตะฝั ะทะฐะบััั")
            return None, 0

        try:
            # ะคะพัะผะธััะตะผ ัะพะพะฑัะตะฝะธั
            messages = [{"role": "system", "content": system_prompt or self.system_prompt}]

            # ะะพะฑะฐะฒะปัะตะผ ะบะพะฝัะตะบัั ะตัะปะธ ะตััั
            if context:
                # ะัะปะธ context - ััะพ ัะฟะธัะพะบ ัะปะพะฒะฐัะตะน ั ะธััะพัะธะตะน
                if isinstance(context, list):
                    for msg in context[-10:]:  # ะะตัะตะผ ะฟะพัะปะตะดะฝะธะต 10 ัะพะพะฑัะตะฝะธะน
                        if isinstance(msg, dict) and "role" in msg and "content" in msg:
                            messages.append({"role": msg["role"], "content": msg["content"]})
                # ะัะปะธ context - ััะพ ัััะพะบะฐ (ััะฐััะน ัะพัะผะฐั)
                elif isinstance(context, str):
                    for line in context.split("\n"):
                        if ": " in line:
                            role, content = line.split(": ", 1)
                            if role.lower() in ["user", "assistant"]:
                                messages.append({"role": role.lower(), "content": content})

            # ะะพะฑะฐะฒะปัะตะผ ัะตะบััะตะต ัะพะพะฑัะตะฝะธะต
            messages.append({"role": "user", "content": user_message})

            # ะะตะปะฐะตะผ ะทะฐะฟัะพั ะบ API
            if not self.client:
                logger.error("OpenAI ะบะปะธะตะฝั ะฝะต ะธะฝะธัะธะฐะปะธะทะธัะพะฒะฐะฝ")
                return None, 0

            # ะัะฟะพะปัะทัะตะผ ะฝะพะฒัะน ะบะปะธะตะฝั
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=1,
                frequency_penalty=0.3,
                presence_penalty=0.3,
            )

            # ะะทะฒะปะตะบะฐะตะผ ะพัะฒะตั
            answer = response.choices[0].message.content
            tokens_used = response.usage.total_tokens

            logger.info(f"OpenAI ะพัะฒะตั ะฟะพะปััะตะฝ. ะขะพะบะตะฝะพะฒ ะธัะฟะพะปัะทะพะฒะฐะฝะพ: {tokens_used}")
            return answer, tokens_used

        except Exception as e:
            logger.error(f"ะัะธะฑะบะฐ ะฟัะธ ะทะฐะฟัะพัะต ะบ OpenAI API: {e}")
            return None, 0

    async def get_scenario_response(
        self, scenario: str, context: Optional[str] = None, user_data: Optional[Dict] = None
    ) -> Tuple[Optional[str], int]:
        """
        ะะพะปััะฐะตั ะพัะฒะตั ะดะปั ะบะพะฝะบัะตัะฝะพะณะพ ััะตะฝะฐัะธั

        Args:
            scenario: ะขะธะฟ ััะตะฝะฐัะธั
            context: ะะพะฟะพะปะฝะธัะตะปัะฝัะน ะบะพะฝัะตะบัั
            user_data: ะะฐะฝะฝัะต ะพ ะฟะพะปัะทะพะฒะฐัะตะปะต

        Returns:
            ะะพััะตะถ (ะพัะฒะตั, ะบะพะปะธัะตััะฒะพ ัะพะบะตะฝะพะฒ)
        """
        # ะัะพะผะฟัั ะดะปั ัะฐะทะฝัั ััะตะฝะฐัะธะตะฒ
        scenario_prompts = {
            "plan": """ะะพะผะพะณะธ ัะพััะฐะฒะธัั ัััะตะบัะธะฒะฝัะน ะฟะปะฐะฝ ะฝะฐ ะดะตะฝั.
            ะฃัะธััะฒะฐะน:
            - ะัะธะพัะธัะตัั ะทะฐะดะฐั
            - ะัะตะผั ะฝะฐ ะพัะดัั
            - ะญะฝะตัะณะตัะธัะตัะบะธะต ะฟะธะบะธ
            - ะะตะฐะปะธััะธัะฝัะต ะฒัะตะผะตะฝะฝัะต ัะฐะผะบะธ
            
            ะกัััะบัััะธััะน ะพัะฒะตั ะฟะพ ะฑะปะพะบะฐะผ ะฒัะตะผะตะฝะธ.""",
            "motivation": """ะะฐะน ะผะพัะธะฒะธััััะธะน ัะพะฒะตั ะฝะฐ ัะตะณะพะดะฝั.
            ะะบะปััะธ:
            - ะะดะพัะฝะพะฒะปััััั ะผััะปั
            - ะะพะฝะบัะตัะฝะพะต ะดะตะนััะฒะธะต
            - ะะฐะฟะพะผะธะฝะฐะฝะธะต ะพ ะฟัะพะณัะตััะต
            
            ะัะดั ะฟะพะทะธัะธะฒะฝัะผ ะฝะพ ัะตะฐะปะธััะธัะฝัะผ.""",
            "failure": """ะะพะผะพะณะธ ะฟัะพะฐะฝะฐะปะธะทะธัะพะฒะฐัั ะฝะตัะดะฐัั ะธ ะธะทะฒะปะตัั ััะพะบะธ.
            ะคะพะบััะธััะนัั ะฝะฐ:
            - ะะฑัะตะบัะธะฒะฝะพะผ ะฐะฝะฐะปะธะทะต ะฑะตะท ัะฐะผะพะบัะธัะธะบะธ
            - ะะพะฝะบัะตัะฝัั ััะพะบะฐั
            - ะะปะฐะฝะต ะดะตะนััะฒะธะน ะฝะฐ ะฑัะดััะตะต
            - ะะพะดะดะตัะถะบะต ะธ ะผะพัะธะฒะฐัะธะธ""",
            "habits": """ะะฐะน ัะพะฒะตั ะฟะพ ัะฐะฑะพัะต ั ะฟัะธะฒััะบะฐะผะธ.
            ะะฐััะผะพััะธ:
            - ะะฐะบ ะฝะฐัะฐัั ะฝะพะฒัั ะฟัะธะฒััะบั
            - ะะฐะบ ะฝะต ัะพัะฒะฐัััั
            - ะกะธััะตะผั ะฒะพะทะฝะฐะณัะฐะถะดะตะฝะธะน
            - ะััะปะตะถะธะฒะฐะฝะธะต ะฟัะพะณัะตััะฐ""",
        }

        # ะะพะปััะฐะตะผ ะฟัะพะผะฟั ะดะปั ััะตะฝะฐัะธั
        scenario_prompt = scenario_prompts.get(scenario, "ะะพะผะพะณะธ ะฟะพะปัะทะพะฒะฐัะตะปั ั ะตะณะพ ะทะฐะฟัะพัะพะผ.")

        # ะคะพัะผะธััะตะผ ัะพะพะฑัะตะฝะธะต
        message = f"{scenario_prompt}\n\nะะพะฝัะตะบัั: {context if context else 'ะะต ัะบะฐะทะฐะฝ'}"

        if user_data:
            message += f"\n\nะะฐะฝะฝัะต ะพ ะฟะพะปัะทะพะฒะฐัะตะปะต: {user_data}"

        return await self.get_chat_response(message, temperature=0.8)

    async def generate_json_response(
        self,
        prompt: str,
        json_schema: Optional[Dict] = None,
        max_tokens: int = 4000,
        temperature: float = 0.2,
        continue_if_truncated: bool = True,
        system_prompt: Optional[str] = None,
    ) -> Tuple[Optional[Dict], str, int]:
        """
        ะะตะฝะตัะธััะตั ะพัะฒะตั ะฒ ัะพัะผะฐัะต JSON ั ัััะพะณะพะน ััะตะผะพะน

        Args:
            prompt: ะัะพะผะฟั ะดะปั ะณะตะฝะตัะฐัะธะธ
            json_schema: JSON ััะตะผะฐ (ะดะปั response_format)
            max_tokens: ะะฐะบัะธะผัะผ ัะพะบะตะฝะพะฒ
            temperature: ะขะตะผะฟะตัะฐัััะฐ ะณะตะฝะตัะฐัะธะธ (0.2 ะดะปั ะดะตัะตัะผะธะฝะธัะพะฒะฐะฝะฝะพััะธ)
            continue_if_truncated: ะะพะทะฐะณััะถะฐัั ะปะธ ะฟัะธ ะพะฑัะตะทะบะต
            system_prompt: ะะฐััะพะผะฝัะน ัะธััะตะผะฝัะน ะฟัะพะผะฟั (ะพะฟัะธะพะฝะฐะปัะฝะพ)

        Returns:
            ะะพััะตะถ (ัะฐัะฟะฐััะตะฝะฝัะน JSON, finish_reason, ะธัะฟะพะปัะทะพะฒะฐะฝะฝัะต ัะพะบะตะฝั)
        """
        if not self.is_configured or self._client_closed:
            logger.error("OpenAI ะฝะต ะฝะฐัััะพะตะฝ ะธะปะธ ะบะปะธะตะฝั ะทะฐะบััั")
            return None, "error", 0

        try:
            # ะัะฟะพะปัะทัะตะผ ะบะฐััะพะผะฝัะน ะธะปะธ ะดะตัะพะปัะฝัะน ัะธััะตะผะฝัะน ะฟัะพะผะฟั ะดะปั JSON
            if system_prompt:
                json_system_prompt = system_prompt
            else:
                json_system_prompt = """You are a JSON API that returns only valid JSON without any additional text.
Your response must be a parseable JSON object.
Do not add comments, explanations, markdown formatting, or any text outside the JSON structure.
Return ONLY the JSON object."""

            messages = [
                {"role": "system", "content": json_system_prompt},
                {"role": "user", "content": prompt},
            ]

            # ะะพะดะณะพัะฐะฒะปะธะฒะฐะตะผ ะฟะฐัะฐะผะตััั ะทะฐะฟัะพัะฐ
            request_params = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": 1,
            }

            # ะััะฐะตะผัั ะธัะฟะพะปัะทะพะฒะฐัั ัััะพะณะธะน JSON ัะตะถะธะผ
            response_format_applied = False

            # ะะฐัะธะฐะฝั 1: JSON Schema (ะตัะปะธ ััะตะผะฐ ะฟัะตะดะพััะฐะฒะปะตะฝะฐ)
            if json_schema:
                try:
                    # ะัะพะฑัะตะผ response_format ั json_schema
                    request_params["response_format"] = {
                        "type": "json_schema",
                        "json_schema": json_schema,
                    }
                    response_format_applied = True
                    logger.info("ะัะฟะพะปัะทัะตะผ response_format ั JSON Schema")
                except Exception as e:
                    logger.warning(f"JSON Schema ะฝะต ะฟะพะดะดะตัะถะธะฒะฐะตััั: {e}")

            # ะะฐัะธะฐะฝั 2: ะัะพััะพะน json_object
            if not response_format_applied:
                try:
                    request_params["response_format"] = {"type": "json_object"}
                    response_format_applied = True
                    logger.info("ะัะฟะพะปัะทัะตะผ response_format ั json_object")
                except Exception as e:
                    logger.warning(f"response_format ะฝะต ะฟะพะดะดะตัะถะธะฒะฐะตััั: {e}")

            # ะะตะปะฐะตะผ ะทะฐะฟัะพั
            response = await self.client.chat.completions.create(**request_params)

            content = response.choices[0].message.content
            finish_reason = response.choices[0].finish_reason
            tokens_used = response.usage.total_tokens if hasattr(response, "usage") else 0

            logger.info(
                f"ะะพะปััะตะฝ ะพัะฒะตั ะพั OpenAI: finish_reason={finish_reason}, tokens={tokens_used}, ะดะปะธะฝะฐ={len(content)}"
            )

            # ะะฑัะฐะฑะพัะบะฐ ะพะฑัะตะทะบะธ ะพัะฒะตัะฐ
            if finish_reason == "length" and continue_if_truncated:
                logger.warning("ะัะฒะตั ะพะฑัะตะทะฐะฝ ะฟะพ ะดะปะธะฝะต, ะฟััะฐะตะผัั ะดะพะทะฐะณััะทะธัั...")

                # ะััะฐะตะผัั ะฝะฐะนัะธ ะฟะพัะปะตะดะฝะธะน ะฟะพะปะฝัะน ะดะตะฝั ะฒ JSON
                try:
                    # ะะฐัะพะดะธะผ ะฟะพะทะธัะธั ะฟะพัะปะตะดะฝะตะณะพ ะทะฐะบัััะพะณะพ ะพะฑัะตะบัะฐ ะดะฝั
                    last_complete_day = content.rfind("},")
                    if last_complete_day > 0:
                        # ะะฑัะตะทะฐะตะผ ะดะพ ะฟะพัะปะตะดะฝะตะณะพ ะฟะพะปะฝะพะณะพ ะดะฝั
                        truncated_json = content[: last_complete_day + 1]

                        # ะััะฐะตะผัั ัะฐัะฟะฐััะธัั, ััะพะฑั ะฟะพะฝััั ัะบะพะปัะบะพ ะดะฝะตะน ัะถะต ะตััั
                        partial_data = None
                        try:
                            # ะัะตะผะตะฝะฝะพ ะทะฐะบััะฒะฐะตะผ JSON ะดะปั ะฟะฐััะธะฝะณะฐ
                            test_json = (
                                truncated_json + "]}"
                                if '"days"' in truncated_json
                                else truncated_json + "}"
                            )
                            partial_data = json.loads(test_json)
                            existing_days = len(partial_data.get("days", []))
                        except:
                            existing_days = 0

                        # ะะฐะฟัะฐัะธะฒะฐะตะผ ะฟัะพะดะพะปะถะตะฝะธะต
                        continuation_prompt = f"""Continue the JSON plan from day {existing_days + 1}.
Return ONLY the remaining days in this exact format:
{{"day": {existing_days + 1}, "tasks": [{{"time": "HH:MM", "activity": "..."}}]}},
{{"day": {existing_days + 2}, "tasks": [{{"time": "HH:MM", "activity": "..."}}]}}
Do not repeat previous days. Return only JSON, no other text."""

                        continuation_params = {
                            "model": self.model,
                            "messages": [
                                {"role": "system", "content": json_system_prompt},
                                {"role": "user", "content": continuation_prompt},
                            ],
                            "temperature": temperature,
                            "max_tokens": 2000,
                            "top_p": 1,
                        }

                        if response_format_applied:
                            continuation_params["response_format"] = {"type": "json_object"}

                        continuation_response = await self.client.chat.completions.create(
                            **continuation_params
                        )
                        continuation_content = continuation_response.choices[0].message.content
                        tokens_used += (
                            continuation_response.usage.total_tokens
                            if hasattr(continuation_response, "usage")
                            else 0
                        )

                        # ะกะบะปะตะธะฒะฐะตะผ JSON
                        if continuation_content:
                            # ะะทะฒะปะตะบะฐะตะผ ัะพะปัะบะพ ะผะฐััะธะฒ days ะธะท ะฟัะพะดะพะปะถะตะฝะธั
                            try:
                                cont_json = json.loads(continuation_content)
                                if isinstance(cont_json, dict) and "days" in cont_json:
                                    # ะะพะฑะฐะฒะปัะตะผ ะฝะพะฒัะต ะดะฝะธ ะบ ัััะตััะฒัััะธะผ
                                    if partial_data:
                                        partial_data["days"].extend(cont_json["days"])
                                        return partial_data, "complete", tokens_used
                            except:
                                pass

                            # ะััะฐะตะผัั ัะบะปะตะธัั ะบะฐะบ ัััะพะบะธ
                            content = truncated_json + "," + continuation_content.strip()
                            if not content.endswith("]}"):
                                content = content.rstrip("}]") + "]}"

                        logger.info(f"ะะพะทะฐะณััะทะบะฐ ะทะฐะฒะตััะตะฝะฐ, ะพะฑัะฐั ะดะปะธะฝะฐ: {len(content)}")

                except Exception as e:
                    logger.error(f"ะัะธะฑะบะฐ ะฟัะธ ะดะพะทะฐะณััะทะบะต: {e}")

            # ะะฐััะธะผ JSON
            if content:
                # ะะฐะทะพะฒะฐั ะพัะธััะบะฐ
                content = content.strip()

                # ะฃะฑะธัะฐะตะผ markdown ะฑะปะพะบะธ ะตัะปะธ ะตััั
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0]

                try:
                    result = json.loads(content)
                    return result, finish_reason, tokens_used
                except json.JSONDecodeError as e:
                    logger.error(f"ะัะธะฑะบะฐ ะฟะฐััะธะฝะณะฐ JSON: {e}")
                    logger.debug(f"ะัะพะฑะปะตะผะฝัะน JSON (ะฟะตัะฒัะต 500 ัะธะผะฒะพะปะพะฒ): {content[:500]}")
                    # ะัะดะตั ะพะฑัะฐะฑะพัะฐะฝะพ ะฒ plan_generator.py
                    return None, finish_reason, tokens_used

            return None, finish_reason, tokens_used

        except Exception as e:
            logger.error(f"ะัะธะฑะบะฐ ะฒ generate_json_response: {e}")
            return None, "error", 0

    async def send_message(
        self,
        message: str,
        context: Optional[List[Dict]] = None,
        scenario: Optional[str] = None,
        max_tokens: int = 500,
        system_prompt: Optional[str] = None,
    ) -> Dict[str, any]:
        """
        ะฃะฝะธะฒะตััะฐะปัะฝัะน ะผะตัะพะด ะดะปั ะพัะฟัะฐะฒะบะธ ัะพะพะฑัะตะฝะธะน

        Args:
            message: ะกะพะพะฑัะตะฝะธะต ะฟะพะปัะทะพะฒะฐัะตะปั
            context: ะะพะฝัะตะบัั/ะธััะพัะธั (ัะฟะธัะพะบ ัะพะพะฑัะตะฝะธะน)
            scenario: ะกัะตะฝะฐัะธะน (ะตัะปะธ ะทะฐะดะฐะฝ)
            max_tokens: ะะฐะบัะธะผัะผ ัะพะบะตะฝะพะฒ
            system_prompt: ะะฐััะพะผะฝัะน ัะธััะตะผะฝัะน ะฟัะพะผะฟั

        Returns:
            ะกะปะพะฒะฐัั ั ัะตะทัะปััะฐัะพะผ
        """
        # ะัะปะธ API ะฝะต ะฝะฐัััะพะตะฝ, ะฒะพะทะฒัะฐัะฐะตะผ ะดะตะผะพ-ะพัะฒะตัั
        if not self.is_configured:
            return self._get_demo_response(scenario)

        try:
            if scenario:
                # ะัะฟะพะปัะทัะตะผ ััะตะฝะฐัะธะน
                response_text, tokens = await self.get_scenario_response(
                    scenario=scenario, context=message
                )
            else:
                # ะะฑััะฝัะน ัะฐั
                response_text, tokens = await self.get_chat_response(
                    user_message=message,
                    context=context,
                    max_tokens=max_tokens,
                    system_prompt=system_prompt,
                )

            if response_text:
                logger.debug(
                    f"ะะพะปััะตะฝ ะพัะฒะตั ะพั OpenAI (ะฟะตัะฒัะต 200 ัะธะผะฒะพะปะพะฒ): {response_text[:200]}..."
                )
                return {
                    "success": True,
                    "response": response_text,
                    "content": response_text,  # ะดะปั ัะพะฒะผะตััะธะผะพััะธ
                    "tokens_used": tokens,
                    "is_demo": False,
                }
            else:
                logger.error("ะัััะพะน ะพัะฒะตั ะพั OpenAI API")
                return {
                    "success": False,
                    "response": "ะะต ัะดะฐะปะพัั ะฟะพะปััะธัั ะพัะฒะตั ะพั AI. ะะพะฟัะพะฑัะนัะต ะฟะพะทะถะต.",
                    "error": "NO_RESPONSE",
                    "is_demo": False,
                }

        except Exception as e:
            logger.error(f"ะัะธะฑะบะฐ ะฒ send_message: {e}")
            return {
                "success": False,
                "response": f"ะัะพะธะทะพัะปะฐ ะพัะธะฑะบะฐ: {str(e)}",
                "error": str(e),
                "is_demo": False,
            }

    def _get_demo_response(self, scenario: Optional[str] = None) -> Dict[str, any]:
        """ะะพะทะฒัะฐัะฐะตั ะดะตะผะพ-ะพัะฒะตัั ะบะพะณะดะฐ API ะฝะตะดะพัััะฟะตะฝ"""
        demo_responses = {
            "plan_day": """๐ **ะะปะฐะฝ ะฝะฐ ะดะตะฝั**
                
ะฃััะพ (9:00-12:00):
โข โ ะัะฟะพะปะฝะธัั ะฒะฐะถะฝัะต ะทะฐะดะฐัะธ
โข โ ะะตัะตััะฒ 15 ะผะธะฝัั
โข ๐ง ะัะพะฒะตัะธัั ะฟะพััั

ะะตะฝั (12:00-17:00):
โข ๐ฝ ะะฑะตะด
โข ๐ฏ ะคะพะบัั-ัะตััะธะธ
โข ๐ ะัััะตัะธ

ะะตัะตั (17:00-21:00):
โข ๐ ะะบัะธะฒะฝะพััั
โข ๐ ะะฑััะตะฝะธะต
โข ๐ง ะัะดัั""",
            "motivation": """๐ช **ะะพัะธะฒะฐัะธั ะดะฝั**
                
ะขั ัะฟะพัะพะฑะตะฝ ะฝะฐ ะฑะพะปััะตะต, ัะตะผ ะดัะผะฐะตัั!

๐ ะะฐะถะดัะน ัะฐะณ ะฟัะธะฑะปะธะถะฐะตั ะบ ัะตะปะธ.
๐ฏ ะคะพะบััะธััะนัั ะฝะฐ ะณะปะฐะฒะฝะพะผ.
๐ซ ะะตัั ะฒ ัะตะฑั!

ะะตะนััะฒัะน! ๐""",
            "default": """๐ค **ะััะธััะตะฝั ะฒ ะดะตะผะพ-ัะตะถะธะผะต**
                
ะะปั ะฟะพะปะฝะพะน ััะฝะบัะธะพะฝะฐะปัะฝะพััะธ ะฝัะถะฝะพ ะฝะฐัััะพะธัั OpenAI API.

ะ ะฟะพะบะฐ ะธัะฟะพะปัะทัะนัะต ะดััะณะธะต ััะฝะบัะธะธ ะฑะพัะฐ:
โข ๐ ะขัะตะบะตั ะฟัะธะฒััะตะบ
โข ๐ฏ ะคะพะบัั-ัะตััะธะธ
โข โ ะงะตะบ-ะปะธัั ะทะฐะดะฐั
โข ๐ค ะัะพัะธะปั ะธ ะดะพััะธะถะตะฝะธั""",
        }

        response = demo_responses.get(scenario, demo_responses["default"])

        return {"success": True, "response": response, "is_demo": True}


# ะะปะพะฑะฐะปัะฝัะน ัะบะทะตะผะฟะปัั ะดะปั ะพะฑัะฐัะฝะพะน ัะพะฒะผะตััะธะผะพััะธ
assistant = OpenAIAssistant()
